---
title: "Clase Modelado 1"
lang: es
date: today
format:
  pdf:
    latex_engine: xelatex
nocite: |
  @*
cache : true
---

# Datos

```{r, message=FALSE, warning=FALSE}
# Corremos librerías
library(dplyr)
library(ggplot2)
library(purrr)
library(MASS)
library(fitdistrplus)
library(actuar)

# Datos
# Dataframe con la severidad de los siniestros
sevDatos <- data.frame(
  Rango = c("0-1000", "1000-3000", "3000-5000", "5000-10000", "+10000"),
  Frecuencia = c(189, 36, 5, 5, 6)
)

sevDatos
# Dataframe con la frecuencia de los siniestros
frecDatos <- data.frame(
  Rango = c("[0,1]", "[2,3]", "[3,4]"),
  Completa = c(56, 12, 2)
)

frecDatos

# Crear un dataframe con los datos de la imagen
rango <- c("[0-.05]", "(0.05-0.1]", "(0.1-0.15]", "(0.15-0.2]", "(0.2-0.25]", 
           "(0.25-0.3]", "(0.3-0.35]", "(0.35-0.4]", "(0.4-0.45]", "(0.45-0.5]", 
           "(0.5-0.55]", "(0.55-0.6]", "(0.6-0.65]", "(0.65-0.7]", "(0.7-0.75]", 
           "(0.75-0.8]", "(0.8-0.85]", "(0.85-0.9]", "(0.9-0.95]", "(0.95-1]")

frecuencia <- c(189, 36, 5, 0, 5, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2)


# Crear el dataframe
sevDatos <- data.frame(Rango = factor(rango, levels = rango), Frecuencia = frecuencia)

# Mostrar el dataframe
```


## Análisis exploratorio de datos

### Siniestralidad


Con lo cual notamos que para la siniestralidad de estos datos, los modelos que convienen deben de ser colas pesadas o medio pesadas, dado que se ve un ligero incremento final en la siniesralidad..

### Frecuencia

```{r}
# Histograma de la frecuencia

# Calcular los porcentajes
frecDatos$Porcentaje <- frecDatos$Completa / sum(frecDatos$Completa) * 100

# Definir los niveles deseados en el orden correcto

niveles_deseados <- c("[0,1]", "[2,3]", "[3,4]")

# Convertir Rango a factor con el orden deseado

frecDatos$Rango <- factor(frecDatos$Rango, levels = niveles_deseados)

# Crear la gráfica con ggplot2

(plot2 <- ggplot(frecDatos, aes(x = Rango, y = Porcentaje)) +
  geom_bar(stat = "identity") +
  labs(title = "Datos agrupados de la frecuencia de siniestros al año",
       x = "Frecuencia de siniestros",
       y = "Porcentaje del total de casos") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)))

# Guardar el gráfico en un archivo JPG

ggsave(filename = "histograma_frecuencia_porc.jpg", plot = plot2, device = "jpeg")
```

Dada la forma de los datos de frecuencia, los modelos que mejor pueden convenir son los modelos de Poisson o Binomial Negativa de la clase(a,b,0) para mantenerlo parsiarmónico. 

## Ajuste modelos de frecuencia 

```{r, message=FALSE, warning=FALSE}
library(stats4)
# Definir los límites de los intervalos
intervalos <- list(c(-1, 1), c(1, 3), c(3,4))

# Crear una función de log-verosimilitud para la distribución de Poisson
logverosimilitud_poisson <- function(lambda) {
  result <- -sum(sapply(1:nrow(frecDatos), function(i) {
    frecDatos$Completa[i] * log(ppois(intervalos[[i]][2], lambda)-ppois(intervalos[[i]][1], lambda))
  }))
  return(result)
}

# Crear una función de log-verosimilitud para la distribución binomial negativa
logverosimilitud_negbin <- function(par) {
  size <- par[1]
  mu <- par[2]
  result <- -sum(sapply(1:nrow(frecDatos), function(i) {
    frecDatos$Completa[i] *log(pnbinom(intervalos[[i]][2], size = size, mu = mu)-pnbinom(intervalos[[i]][1], size = size, mu = mu))
  }))
  return(result)
}


# Ajuste a la distribución de Poisson
ajuste_poisson <- optimize(interval = c(.2,10), f = logverosimilitud_poisson)
(lambda_poisson <- ajuste_poisson$minimum)

# Ajuste a la distribución binomial negativa
ajuste_negbin <- optim(par = c(1, 1), fn = logverosimilitud_negbin)
size_negbin <- ajuste_negbin$par[1]
(mu_negbin <- ajuste_negbin$par[2])


# AIC()

# AIC para Poisson
(AIC_poisson <- 2 * logverosimilitud_poisson(lambda_poisson) + 2)

# AIC para binomial negativa
(AIC_negbin <- 2 * logverosimilitud_negbin(c(size_negbin, mu_negbin)) + 4)

# Prueba de chi-cuadrado
observados <- frecDatos$Completa

# Esperados para Poisson
esperados_poisson <- sapply(1:nrow(frecDatos), function(i) {
  (ppois(intervalos[[i]][2], lambda_poisson)-ppois(intervalos[[i]][1], lambda_poisson))*sum(observados)
})


# Esperados para binomial negativa
esperados_negbin <- sapply(1:nrow(frecDatos), function(i) {
  (pnbinom(intervalos[[i]][2], size = size_negbin, mu = mu_negbin)-pnbinom(intervalos[[i]][1], size = size_negbin, mu = mu_negbin))*sum(observados)
})


# Prueba de bonda de ajuste

# Chi-cuadrado para Poisson
chisq_poisson <- sum((observados - esperados_poisson)^2 / esperados_poisson)
p_val_poisson <- pchisq(chisq_poisson, df = nrow(frecDatos) - 1, lower.tail = FALSE)

# Chi-cuadrado para binomial negativa
chisq_negbin <- sum((observados - esperados_negbin)^2 / esperados_negbin)
p_val_negbin <- pchisq(chisq_negbin, df = nrow(frecDatos)- 2, lower.tail = FALSE)

# Resultados de la prueba de chi-cuadrado
cat("Chi-cuadrado para Poisson:", chisq_poisson, "p-value:", p_val_poisson, "\n")
cat("Chi-cuadrado para Binomial Negativa:", chisq_negbin, "p-value:", p_val_negbin, "\n")
```

Notamos que al ajustar, se valida la bondad de ajuste de los modelos de Poisson y Binomial Negativa; sin embargo, el modelo de Poisson es el que tiene un mejor AIC, parámetro que usaremos para elegir el modelo, dado que nos indica que realiza un mejor ajuste a los datos.

Por último, solo mostraremos los resultados del ajuste gráficamente.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)

# Crear un data frame con todos los datos
frecDatosComp <- data.frame(
  Rango = factor(frecDatos$Rango),
  Observados = observados,
  Esperados_Poisson = esperados_poisson,
  Esperados_NegBin = esperados_negbin
)

# Convertir los datos a formato largo para ggplot2
frecDatos_long <- reshape2::melt(frecDatosComp, id.vars = "Rango", 
                                 variable.name = "Tipo", value.name = "Frecuencia")

# Crear la gráfica de barras
plot3 <- ggplot(frecDatos_long, aes(x = Rango, y = Frecuencia, fill = Tipo)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparación de ajustes de distribución de frecuencias",
       x = "Rango",
       y = "Frecuencia") +
  scale_fill_manual(values = c("Observados" = "blue", 
                               "Esperados_Poisson" = "red", 
                               "Esperados_NegBin" = "green"))

# Guardar el gráfico en un archivo JPG
ggsave(filename = "ajuste_frecuencia.jpg", plot = plot3, device = "jpeg")

```

## Ajuste modelos de siniestralidad

Procedimos a ajustar los modelos weibull, gamma, lognormal y pareto a los datos de siniestralidad. Y con los cuales obtuvimos la siguiente gráfica para comparar el ajuste, además de que realizamos una prueba Chi-cuadrado para validar la bondad de ajuste de los modelos.


```{r, message=FALSE, warning=FALSE}
# Ajuste de los datos
library(fitdistrplus)
library(ggplot2)
library(readxl)
# Corregir los datos

set.seed(191654)

limites <- list(c(0, 0.05), c(0.05, 0.1), c(0.1, 0.15), c(0.15, 0.2), c(0.2, 0.25), 
             c(0.25, 0.3), c(0.3, 0.35), c(0.35, 0.4), c(0.4, 0.45), c(0.45, 0.5), 
             c(0.5, 0.55), c(0.55, 0.6), c(0.6, 0.65), c(0.65, 0.7), c(0.7, 0.75), 
             c(0.75, 0.8), c(0.8, 0.85), c(0.85, 0.9), c(0.9, 0.95), c(0.95, 1))



logverosimilitud_beta <- function(params) {
  shape1 <- params[1]
  shape2 <- params[2]
  result <- -sum(sapply(1:nrow(sevDatos), function(i) {
    sevDatos$Frecuencia[i] * log(pbeta(limites[[i]][2], shape1 = shape1, shape2 = shape2) - 
                                 pbeta(limites[[i]][1], shape1 = shape1, shape2 = shape2))
  }))
  return(result)
}

pmezclaBeta <- function(x, shape1, shape2, shape3, shape4, p) {
  p * pbeta(x, shape1, shape2) + (1 - p) * pbeta(x, shape3, shape4)
}


logverosimilitud_mezcla_beta <- function(params) {
  shape1 <- params[1]
  shape2 <- params[2]
  shape3 <- params[3]
  shape4 <- params[4]
  p <- params[5]
  result <- -sum(sapply(1:nrow(sevDatos), function(i) {
    sevDatos$Frecuencia[i] * log(pmezclaBeta(limites[[i]][2], shape1 = shape1, shape2 = shape2, shape3 = shape3, shape4 = shape4, p) - 
    pmezclaBeta(limites[[i]][1], shape1 = shape1, shape2 = shape2, shape3 = shape3, shape4 = shape4, p))
  }))
  return(result)
}


# Ajustar las distribuciones utilizando optimización
ajustar_distribucion <- function(logverosimilitud, start_params) {
  fit <- optim(start_params, logverosimilitud)
  return(fit)
}

# Ajustar las distribuciones
params_beta <- ajustar_distribucion(logverosimilitud_beta, start_params = c(.5, .5))


# Calcular AIC
(AIC_beta <- 2 * logverosimilitud_beta(params_beta$par) + 4)

# Prueba de chi-cuadrada

# Calcular las frecuencias esperadas para cada distribución
calcular_frecuencias_esperadas <- function(distribucion, params, limites, total_observaciones) {
    p <- params$par[5]
    shape3 <- params$par[3]
    shape4 <- params$par[4]
    shape1 <- params$par[1]
    shape2 <- params$par[2]
  p <- sapply(1:length(limites), function(i) {
    if (distribucion == "beta") {
      return(pbeta(limites[[i]][2], shape1 = params$par[1], shape2 = params$par[2]) - pbeta(limites[[i]][1],  shape1 = params$par[1], shape2 = params$par[2]))
    } else {
      stop("Distribución no soportada")
    }
  })
  return(p * total_observaciones)
}       


(total_observaciones <- sum(sevDatos$Frecuencia))

expected_beta <- calcular_frecuencias_esperadas("beta", params_beta, limites, total_observaciones)


# Realizar la prueba chi cuadrado para cada distribución
observed <- sevDatos$Frecuencia

(chisq_test_beta <- chisq.test(x = observed, p = expected_beta / sum(expected_beta), rescale.p = TRUE, simulate.p.value = TRUE))


# Crear un data frame con las frecuencias observadas y esperadas
comparacion <- data.frame(
  Rango = rep(sevDatos$Rango, 2),
  Frecuencia = c(sevDatos$Frecuencia, expected_beta),
  Tipo = rep(c("Observada", "beta"), each = nrow(sevDatos))
)

# Graficar las frecuencias observadas y esperadas
ggplot(comparacion, aes(x = Rango, y = Frecuencia, fill = Tipo)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparación de ajustes de siniestralidad",
       x = "Rangos de siniestralidad",
       y = "Frecuencia",
       fill = "Distribución") +
  theme_minimal() +
  scale_fill_manual(values = c("Observada" = "black", "beta" = "gray"))+
    theme(axis.text.x = element_text(angle = 90, hjust = 1))


```


Con lo cual evaluando las pruebas de bondad de ajuste, la gráfica comparativa y la métrica de AIC, notamos que el modelo que mejor se ajusta a los datos es el modelo de Pareto, por lo que es el modelo que usaremos para representar siniestralidad.


# Prima de riesgo

## Supuestos

El modelo de severidad agregada para calcular la prima de riesgo promedio y la cuota de riesgo fue el siguiente:

$$S = \sum_{i=1}^{N} W_{i}$$

Donde 

- $W_{i}$ es la severidad (porcentaje de la siniestralidad con respecto al valor del intrumentos) de los siniestros ocurridos. Esta variable es aleatoria. 

- $N$ es el número de eventos que ocurrirán en el año. E igualmente es una variable aleatoria.

Suponemos que las 2 variables son independientes entre sí.

Dado este modelo de riesgo colectivo presentado, procedemos a calcular la prima de riesgo promedio y la cuota de riesgo.

La prima de riesgo promedio para este seguro de daños de instrumentos musicales se calcula como la esperanza del modelo:

$$E[S] = E[\sum_{i=1}^{N} W_{i}] = E[N] E[W]=E[N]E[W]$$

Donde $E[N]$ es la esperanza del número de eventos, y $E[VW]=E[V]E[W]$ es la siniestralidad esperada que tendríamos para una cartera equilibrada de instrumentos musicales.


Ahora bien, para calcular esta prima de riesgo recurrimos numéricamente a simular los valores de nuestro modelo de riesgo colectivo, de acuerdo con modelos ajustados de la sección anterior para las variables aleatorias $N$ y $W$. 

Con lo cual, para nosotros nuestro riesgo de siniestralidad está medido de acuerdo con la siguiente variable $VW = \text{min}((X-2000)_{+}, 100000)$

## Cálculos

Para poder calcular la esperanza utilizaremos técnicas avanzadas de simulación. En este caso utilizaremos el método de bootstrap paramétrico para calcular la esperanza de la siniestralidad.

```{r, message=FALSE, warning=FALSE}
library(fitdistrplus)
library(actuar)  # Para la distribución Pareto
library(parallel)
set.seed(191654)
# Supuestos de los parámetros calculados anteriormente
deducible <- .05   # Deducible para ajustar los valores de VW
lambda_poisson <- lambda_poisson  # Tasa de la distribución Poisson
alpha_beta <- params_beta$par[1]    # Parámetro de forma de la distribución Pareto
beta_beta <- params_beta$par[2]     # Parámetro de escala de la distribución Pareto
# Número de simulaciones
M <- 10000
B <- 10000

# Función de simulación optimizada
simular_S <- function(lambda_poisson, alpha_beta, beta_beta, deducible, M) {
  # 1. Simulaciones de N
  N <- rpois(M, lambda_poisson)
  
  # 2. Simulaciones de VW
  VW <- lapply(N, function(n) {
    if (n == 0) return(0)
    valores <- rbeta(n, shape1 = alpha_beta, shape2 = beta_beta)
    valores[valores > deducible] <- valores[valores > deducible] - deducible
    valores[valores <= deducible] <- 0
    return(valores)
  })
  
  # 3. Construcción de M simulaciones de S
  S <- sapply(VW, sum)
  
  VW <- unlist(VW)
  
  return(list(S = S, N = N, VW = VW))
}

# Función para ejecutar el proceso en paralelo
simular_bootstrap <- function(iter, lambda_poisson, alpha_beta, beta_beta, deducible, M) {
  sim_data <- simular_S(lambda_poisson, alpha_beta, beta_beta, deducible, M)
  mean(sim_data$S)
}

# Configurar clúster para paralelización
cl <- makeCluster(detectCores() - 1)
clusterExport(cl, c("lambda_poisson", "alpha_beta", "beta_beta", "deducible", "M", "simular_S", "rpois", "rbeta", "sapply", "unlist"))

# Ejecutar simulaciones en paralelo
promedios_S <- parSapply(cl, 1:B, simular_bootstrap, lambda_poisson, alpha_beta, beta_beta, deducible, M)

# Detener clúster
#stopCluster(cl)

# Resultados
mean_promedio_S <- mean(promedios_S)
error_estandar_S <- sd(promedios_S)

# Mostrar resultados
cat("Promedio de S (Plugin de la esperanza de S):", mean_promedio_S, "\n")
cat("Error estándar bootstrap del promedio de S:", error_estandar_S, "\n")
```

Es decir, la cuota de riesgo que tendríamos que cobrar a cada instrumento asegurado en nuestra cartera equilibrada sería de 

$$E[VW]/E[V] = \frac{`r mean_promedio_S`}{`r promedioInstrumentos`}$$

`r mean_promedio_S/promedioInstrumentos`.

Para terminar de validar el ajuste presentamos el histograma de la distribución boostrap de la siniestralidad esperada agregada.

```{r, message=FALSE, warning=FALSE}
# Calcular los valores acumulados de los promedios de S
promedios_S_acumulados <- cumsum(promedios_S) / seq_along(promedios_S)

library(ggplot2)

# Crear un data frame con los valores acumulados de los promedios de S
df_acumulados <- data.frame(
  Iteracion = seq_along(promedios_S_acumulados),
  Promedio_Acumulado_S = promedios_S_acumulados
)

# Crear el histograma para los valores acumulados
(plot_acumulado <- ggplot(df_acumulados, aes(x = Promedio_Acumulado_S)) +
  geom_histogram(binwidth = .001, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histograma de los Promedios Acumulados de S",
       x = "Valor Promedio Acumulado de S",
       y = "Frecuencia") +
  theme_minimal())

# Guardar la gráfica en un archivo JPG
ggsave(filename = "histograma_promedios_acumulados_S.jpg", plot = plot_acumulado, device = "jpeg")


```

# RCS

Ahora dado los resultados anteriores debemos de calcular el requerimiento de capital de solvencia (RCS) para la aseguradora. Para ello, debemos de calcular 3 componentes:

$$\textbf{RCS} = \textbf{RCS}_{T} + \textbf{RCS}_{F}+ \textbf{RCS}_{Op}$$

Donde:

- $\textbf{RCS}_{T}$ es el requerimiento de capital de solvencia técnico, que se calcula como el exceso del percentil 99.5 de la distribución de la siniestralidad esperada agregada con respecto a lo esperado. Es decir, $\textbf{RCS}_{T}=q_{0.995}(S)-E(S)$. Esta fórmula es muy simple porque solo tenemos un producto y es lo máximo que podríamos perder en un año con una confianza del 99.5%.

- $\textbf{RCS}_{F}$ es el requerimiento de capital de solvencia financiero. En este caso es cero, porque suponemos que no tendremos riesgos financieros a un año.

- $\textbf{RCS}_{Op}$ es el requerimiento de capital de solvencia operativo, que se calcula como el 15% de la prima de riesgo promedio.

Por lo tanto, para el cálculo del RCS, procedemos a calcular el RCS técnico y el RCS operativo.

```{r, message=FALSE, warning=FALSE}
# Calcular el RCS técnico
# Número de simulaciones
M <- 10000
B <- 10000


# Función para ejecutar el proceso en paralelo
simular_bootstrap <- function(iter, lambda_poisson, alpha_beta, beta_beta, deducible, M) {
  sim_data <- simular_S(lambda_poisson, alpha_beta, beta_beta, deducible, M)
  quantile(sim_data$S, 0.995)
}

# Configurar clúster para paralelización
cl <- makeCluster(detectCores() - 1)
clusterExport(cl, c("lambda_poisson", "alpha_beta", "beta_beta", "deducible", "M", "simular_S", "rpois", "rbeta", "sapply", "unlist", "quantile"))

# Ejecutar simulaciones en paralelo
cuantiles_S <- parSapply(cl, 1:B, simular_bootstrap, lambda_poisson, alpha_beta, beta_beta, deducible, M)

# Detener clúster
#stopCluster(cl)

# Resultados
mean_cuantil_S <- mean(cuantiles_S)
error_estandar_S <- sd(cuantiles_S)

# Mostrar resultados
cat("Promedio de S (Plugin de la esperanza de S):", mean_cuantil_S, "\n")
cat("Error estándar bootstrap del promedio de S:", error_estandar_S, "\n")

# Intervalos:

# Calcular los intervalos de confianza bootstrap utilizando cuantiles
cuantiles_S_Int <- quantile(cuantiles_S, c(0.025, 0.5, .75,0.975))

# Extraer los cuantiles
(cuantil_2.5 <- cuantiles_S_Int[1])
(cuantil_50 <- cuantiles_S_Int[2])
(cuantil_75 <- cuantiles_S_Int[3])
(cuantil_97.5 <- cuantiles_S_Int[4])


#  Gráfica
cuantiles_S_acumulados <- cumsum(cuantiles_S) / seq_along(cuantiles_S)

# Crear un data frame con los valores acumulados de los promedios de S
df_acumulados <- data.frame(
  Iteracion = seq_along(cuantiles_S_acumulados),
  Promedio_Acumulado_S = cuantiles_S_acumulados
)

# Crear el histograma para los valores acumulados
(plot_acumulado <- ggplot(df_acumulados, aes(x = Promedio_Acumulado_S)) +
  geom_histogram(binwidth = 100, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histograma de los Cuantiles Acumulados de S",
       x = "Valor Promedio Acumulado de los cuantiles de S",
       y = "Frecuencia") +
  theme_minimal())

# Guardar la gráfica en un archivo JPG
ggsave(filename = "histograma_cuantiles_acumulados_S.jpg", plot = plot_acumulado, device = "jpeg")

```

Es decir, el RCS técnico que tendríamos que tener para asegurar los riesgos de siniestralidad esperada agregada es de  `r mean_cuantil_S - mean_promedio_S` en promedio por cada póliza. 

Por otro lado, el RCS operativo que tendríamos que tener para asegurar los riesgos de operación es de `r mean_promedio_S * 0.15` por cada póliza.

Por lo tanto, el RCS total que tendríamos que tener para asegurar los riesgos de siniestralidad y operativos es de `r mean_cuantil_S - mean_promedio_S + mean_promedio_S * 0.15` por cada póliza. 


# Tarificación

Para la tarificación de los seguros de instrumentos musicales, se propone una tarifa de acuerdo con la cuota de riesgo calculada anteriormente, pero agregamos gastos administrativos del 10% de la cuota de riesgo, utilidad del 20%, y un margen de seguridad del 20% de la cuota de riesgo.

Por lo tanto, la cuota de tarifa que proponemos para el seguro de instrumentos musicales es de `r mean_promedio_S/(promedioInstrumentos * (1-.5))` por cada instrumento asegurado.

Al final, nos daría una tarifa promedio de `r mean_promedio_S/((1-.5))` por cada instrumento asegurado.

# RRC

Para la RRC, debemos reservar por un año un porcentaje por cada póliza vendida, este porcentaje se calcula de la siguiente forma para cada póliza vendida:

$$RRC\% = \frac{\text{primaRiesgo}}{\text{primaTarifa}} = `r (1-.5)`$$

Suponemos que en todos los años las pólizas se adquieren uniformemente a lo largo del año, y se devengan uniformemente por el plazo del seguro renovable, que es un año. Por lo que en un año, de todo lo que se venda en pólizas se debe reservar la mitad de la prima de riesgo promedio.

# ROPC

Como no tenemos experiencia en el mercado, proponemos un ROPC del 100% de la prima de riesgo promedio, es decir, `r (1-.5) ` por cada póliza vendida. Ya cuando generemos experiencia, podremos ajustar este parámetro.









