---
title: "Clase Modelado 1"
lang: es
date: today
format:
  pdf:
    latex_engine: xelatex
nocite: |
  @*
---


# Bases Técnicas, Estadísticas y Financieras

## Supuestos

a) Tipo de seguro:

Es un seguro de daños materiales. Específicamente para instrumentos musicales.

b) Cobertura

La cobertura principal es daño parcial o total de instrumento por las siguientes causas:

- Daños accidentales
- Robo(daño total)
- Daños por ambiente

c) Temporalidad

1 año con renovación

d) Población asegurada

Instrumentos musicales con valor de más de $1,000.

e) Suma asegurada de 100,000 (SA)

Este límite aplica para todos los siniestros dentro del año.

f) Gastos administrativos

Solo consideramos gastos administrativos del 30% de la prima

g) Deducibles, coaseguro y límites de asseguramiento

El deducible es 0. 

Coaseguro es 30% del siniestro.

El límite de aseguramiento es 100,000 (SA).

h) Modelo de siniestralidad
Para modelar nuestra siniestralidad utilizamos el modelo de riesgos colectivo el cual considera siniestros homogéneos e independientes.
Con lo cual representa la pérdida agrega de acuerdo con la siguiente fórmula:

$$S = \sum_{k=1}^{N} \sum_{j=1}^{N_k}X_j$$

donde

- $S$ es la pérdida agregada
- $N$ es el número de pólizas en la cartera
- $N_k$ es el número de siniestros en la póliza $k$. Suponemos que $N_k$ sigue una distribución __ y en conjunto son vaiid.
- $X_j$ es el monto del siniestro $j$ en la póliza $k$. Suponemos que $X_j$ sigue una distribución __ y en conjunto son vaiid.

Por los supuestos vaiid este modelo se puede simplificar estadísticamente a:

$$S = \sum_{j=1}^{M} X_j$$

donde 

- $M = \sum_{j=1}^{N} N_j$ es el número total de siniestros en la cartera. $M$ sigue una distribución __.
- $X_j$ es el monto del siniestro $j$ en la cartera. Suponemos que $X_j$ sigue una distribución __ y en conjunto son vaiid.

i) Prima de neta riesgo

Con el modelo descrito anteriormente obtenemos la prima de neta de riesgo de la siguiente manera:

$$P = \text{Var}_{.995}(S)(1+\alpha_{sop})(1+\alpha_{op})$$

donde $\alpha_{op} = .1$ es el margen para cubrir riesgos operativos; y $\alpha_{sop}$ es el margen adicional para cubrir gastos administrativos, utiilidades, y un margen extra de seguridad.

En este caso $\alpha_{sop} = \alpha_{ga} + \alpha_{ut} + \alpha_{seg} = .30$

y tenemos que:
- $\alpha_{ga} = .1$ es el margen para cubrir gastos administrativos.
- $\alpha_{ut} = .05$ es el margen para cubrir utilidades.
- $\alpha_{seg} = .05$ es el margen para cubrir rango de seguridad.

Para obtener la prima de neta de riesgo individual calculamos:

$$P_{\text{Ind}} = \frac{P}{N}$$

donde $N$ es el número de pólizas en la cartera.

k) RCS

Por nuestro modelo interno de riesgos aprobados por la comisión el RCS se calcula de la siguiente manera:

$$RCS = (\text{Var}_{.995}(S) - \text{Var}_{.5}(S))+\text{Var}_{.995}(S)(1+\alpha_{op}) = N*P*\left(\frac{1}{(1+\alpha_{sop})}+\frac{}{(1+\alpha_{sop})(1+\alpha_{sop})}\right)$$

j) Reservas de RRC y OPC

RRC

Reservaremos todo el porcentaje de la prima que corresponde a la prima de riesgo los gastos administrativos. Esto se representa con la siguiente fórmula:



OPC





Como tenemos modelo interno aprobado, la RCS

l) Dividendos
No existen dividendos.

m) Valores garantizados.
No existen valores garantizados.

n) Otros aspectos técnicos relevantes.
No existen otros aspectos técnicos relevantes.

## Ajustes
### Ajuste de la distribución de siniestros

Para ajustar la distribución de los siniestros totales utilizamos los siguientes datos, extraídos de ___ . Consideramos las 
siguientes distribuciones:

-
-
-

```{r}

```

En las métricas que utilizamos para elegir la distribución que mejor se ajusta a los datos, obtuvimos los siguientes resultados:

```{r}

```

Por lo que elegimos la distribución ___ para ajustar los siniestros totales.

#### Validaciones

### Ajuste de la distribución del monto de siniestros

Para ajustar la distribución de montos por siniestro utilizamos los siguientes datos, extraídos de ___ . Consideramos las 
siguientes distribuciones:
-
-
-

```{r}

```

En las métricas que utilizamos para elegir la distribución que mejor se ajusta a los datos, obtuvimos los siguientes resultados:

```{r}

```

Por lo que elegimos la distribución ___ para ajustar los siniestros totales.

#### Validaciones



## Resultados



# Modelos

```{r}
library(dplyr)
library(ggplot2)
library(purrr)
library(MASS)
library(fitdistrplus)

library(actuar)

# Datos
# Los que no son 10,000-50,000
# Crear un dataframe con la severidad de los siniestros
sevDatos <- data.frame(
  Rango = c("0-1000", "1000-3000", "3000-5000", "5000-10000", "+10000"),
  Frecuencia = c(79, 37, 4, 6, 7)
)

frecDatos <- data.frame(
  Rango = c("0--1", "2--3", "3--4"),
  Completa = c(56, 12, 2)
)


```

## Modelos de frecuencia

```{r}
library(stats4)
# Definir los límites de los intervalos
intervalos <- list(c(-1, 1), c(1, 3), c(3,4))

# Crear una función de log-verosimilitud para la distribución de Poisson
logverosimilitud_poisson <- function(lambda) {
  result <- -sum(sapply(1:nrow(frecDatos), function(i) {
    frecDatos$Completa[i] * log(ppois(intervalos[[i]][2], lambda)-ppois(intervalos[[i]][1], lambda))
  }))
  return(result)
}

# Crear una función de log-verosimilitud para la distribución binomial negativa
logverosimilitud_negbin <- function(par) {
  size <- par[1]
  mu <- par[2]
  result <- -sum(sapply(1:nrow(frecDatos), function(i) {
    frecDatos$Completa[i] *log(pnbinom(intervalos[[i]][2], size = size, mu = mu)-pnbinom(intervalos[[i]][1], size = size, mu = mu))
  }))
  return(result)
}


# Ajuste a la distribución de Poisson
ajuste_poisson <- optimize(interval = c(.2,10), f = logverosimilitud_poisson)
lambda_poisson <- ajuste_poisson$minimum

# Ajuste a la distribución binomial negativa
ajuste_negbin <- optim(par = c(1, 1), fn = logverosimilitud_negbin)
size_negbin <- ajuste_negbin$par[1]
mu_negbin <- ajuste_negbin$par[2]

```

```{r}
# AIC()

# AIC para Poisson
AIC_poisson <- 2 * logverosimilitud_poisson(lambda_poisson) + 2

# AIC para binomial negativa
AIC_negbin <- 2 * logverosimilitud_negbin(c(size_negbin, mu_negbin)) + 4
```

```{r}

# Prueba de chi-cuadrado
observados <- frecDatos$Completa

# Esperados para Poisson
esperados_poisson <- sapply(1:nrow(frecDatos), function(i) {
  (ppois(intervalos[[i]][2], lambda_poisson)-ppois(intervalos[[i]][1], lambda_poisson))*sum(observados)
})


# Esperados para binomial negativa
esperados_negbin <- sapply(1:nrow(frecDatos), function(i) {
  (pnbinom(intervalos[[i]][2], size = size_negbin, mu = mu_negbin)-pnbinom(intervalos[[i]][1], size = size_negbin, mu = mu_negbin))*sum(observados)
})



# Chi-cuadrado para Poisson
chisq_poisson <- sum((observados - esperados_poisson)^2 / esperados_poisson)
p_val_poisson <- pchisq(chisq_poisson, df = nrow(frecDatos) - 1, lower.tail = FALSE)

# Chi-cuadrado para binomial negativa
chisq_negbin <- sum((observados - esperados_negbin)^2 / esperados_negbin)
p_val_negbin <- pchisq(chisq_negbin, df = nrow(frecDatos)- 2, lower.tail = FALSE)

# Resultados de la prueba de chi-cuadrado
cat("Chi-cuadrado para Poisson:", chisq_poisson, "p-value:", p_val_poisson, "\n")
cat("Chi-cuadrado para Binomial Negativa:", chisq_negbin, "p-value:", p_val_negbin, "\n")
```

```{r}
# Gráfica de ajuste
library(ggplot2)

```


## Modelos de severidad

```{r}
# Cargar el paquete readxl
library(readxl)
mydata <- read_excel("instrumentos.xlsx", sheet = "data")
# Definir una función para calcular el valor intermedio
calcular_intermedio <- function(rango) {
  if (grepl("más de", rango)) {
    lower <- as.numeric(gsub("[^0-9]", "", rango))
    upper <- NA
    intermedio <- lower * 1.5  # Asumimos un incremento del 50% como valor intermedio
  } else {
    nums <- as.numeric(unlist(regmatches(rango, gregexpr("[0-9]+", rango))))
    lower <- nums[1]
    upper <- nums[2]
    intermedio <- mean(c(lower, upper))
  }
  return(intermedio)
}
# Aplicar la función a los datos
mydata$Intermedio <- sapply(mydata$"¿Cuánto cuesta tu instrumento?", calcular_intermedio)

listaSeveridad <- c()
listaMedioSiniestros <- c("500_siniestros" = 500,
                          "2000_siniestros" = 2000, 
                          "4000_siniestros" = 4000, 
                          "7500_siniestros" = 7500, 
                          "12000_siniestros" = 12000)
# Modificamos la lista
mydata <- mydata |> 
  filter(Intermedio>500)
mean(mydata$Intermedio)
```

```{r}

# Bucle para calcular y agregar valores a listaSeveridad
for (i in 1:nrow(mydata)) {
  for (j in c("2000_siniestros", "4000_siniestros", "7500_siniestros", "12000_siniestros")) {
    # Calcular la cantidad a repetir
    cantidad_a_repetir <- mydata[i, j]
    # Calcular el valor a agregar
    valor_a_agregar <- listaMedioSiniestros[j] / mydata$Intermedio[i]
    if (valor_a_agregar>1){
      valor_a_agregar <- 1
    }
    # Agregar los valores a listaSeveridad
    listaSeveridad <- c(listaSeveridad, rep(valor_a_agregar, cantidad_a_repetir))
  }
}

names(listaSeveridad) <- NULL
# Transformar valores de 0 y 1 ligeramente para que estén en el intervalo (0, 1)
eps <- .Machine$double.eps
listaSeveridad[listaSeveridad == 0] <- 3*eps
listaSeveridad[listaSeveridad == 1] <- 1 - 3*eps
# Ajustar una distribución Beta a los datos
fit_beta <- fitdist(listaSeveridad, "beta", method = "mle")

logverosimilitud_beta <- function(par) {
  shape1 <- par[1]
  shape2 <- par[2]
  result <- -sum(sapply(listaSeveridad, function(x) {
    -log(dbeta(x, shape1, shape2)
  }))
  return(result)
}

# Ajustar la distribución Beta
ajuste_beta <- optim(par = c(.3,.5), fn = logverosimilitud_beta)

# Mostrar el resumen del ajuste
summary(fit_beta)

# Graficar el ajuste
plot(fit_beta)

```

## Modelos de siniestralidad


```{r}
library(fitdistrplus)
library(ggplot2)
library(extraDistr)
deducible <- 1000
sevDatos <- sevDatos |> 
  filter(Rango != "0-1000")

# Definir los límites de los rangos
limites <- list( c(1000, 3000), c(3000, 5000), c(5000, 10000), c(10000, 100000))

# Función de log-verosimilitud para la distribución Weibull
logverosimilitud_weibull <- function(par) {
  shape <- par[1]
  scale <- par[2]
  result <- -sum(sapply(1:nrow(sevDatos), function(i) {
    sevDatos$Frecuencia[i] * log(pweibull(limites[[i]][2], shape, scale) - pweibull(limites[[i]][1], shape, scale))
  }))+sum(sevDatos$Frecuencia)*log(pweibull(deducible, shape, scale, lower.tail = FALSE))
  return(result)
} # empezar con c(1, 1000)


# Función de log-verosimilitud para la distribución Gamma
logverosimilitud_gamma <- function(par) {
  shape <- par[1]
  rate <- par[2]
  result <- -sum(sapply(1:nrow(sevDatos), function(i) {
    sevDatos$Frecuencia[i] * log(pgamma(limites[[i]][2], shape, scale = rate) - pgamma(limites[[i]][1], shape, scale = rate))
  }))+sum(sevDatos$Frecuencia)*log(pgamma(deducible, shape, scale = rate, lower.tail = FALSE))
  return(result)
}

# Función de log-verosimilitud para la distribución Lognormal
logverosimilitud_lognormal <- function(par) {
  meanlog <- par[1]
  sdlog <- par[2]
  result <- -sum(sapply(1:nrow(sevDatos), function(i) {
    sevDatos$Frecuencia[i] * log(plnorm(log(limites[[i]][2]), meanlog, sdlog) - plnorm(log(limites[[i]][1]), meanlog, sdlog))
  })) + sum(sevDatos$Frecuencia)*log(plnorm(log(deducible), meanlog, sdlog, lower.tail = FALSE))
  return(result)
}

# Función de log-verosimilitud para la distribución Pareto
logverosimilitud_pareto <- function(par) {
  shape <- par[1]
  scale <- par[2]
  result <- -sum(sapply(1:nrow(sevDatos), function(i) {
    sevDatos$Frecuencia[i] * log(ppareto(limites[[i]][2], shape, scale) - ppareto(limites[[i]][1], shape, scale))
  }))+sum(sevDatos$Frecuencia)*log(ppareto(deducible, shape, scale, lower.tail = FALSE))
  return(result)
}
# Ajustar las distribuciones utilizando optimización
ajustar_distribucion <- function(logverosimilitud, start_params) {
  fit <- optim(start_params, logverosimilitud)
  return(fit)
}

# Ajustar las distribuciones
params_weibull <- ajustar_distribucion(logverosimilitud_weibull, start_params = c(1, 1000))
params_gamma <- ajustar_distribucion(logverosimilitud_gamma, start_params = c(10, 1000))
params_lognormal <- ajustar_distribucion(logverosimilitud_lognormal, start_params = c(1, 1))
params_pareto <- ajustar_distribucion(logverosimilitud_pareto, start_params = c(10, 1000))

# Calcular AIC
AIC_weibull <- 2 * logverosimilitud_weibull(params_weibull$par) + 4
AIC_gamma <- 2 * logverosimilitud_gamma(params_gamma$par) + 4
AIC_lognormal <- 2 * logverosimilitud_lognormal(params_lognormal$par) + 4
AIC_pareto <- 2 * logverosimilitud_pareto(params_pareto$par) + 4

# Prueba de chi-cuadrada

# Calcular las frecuencias esperadas para cada distribución
calcular_frecuencias_esperadas <- function(distribucion, params, limites, total_observaciones) {
  p <- sapply(1:length(limites), function(i) {
    if (distribucion == "weibull") {
      return((pweibull(limites[[i]][2], params$par[1], params$par[2]) - 
              pweibull(limites[[i]][1], params$par[1], params$par[2])) / 
              (1 - pweibull(deducible, params$par[1], params$par[2])))
    } else if (distribucion == "gamma") {
      return((pgamma(limites[[i]][2], params$par[1], rate = params$par[2]) - 
              pgamma(limites[[i]][1], params$par[1], rate = params$par[2])) / 
              (1 - pgamma(deducible, params$par[1], rate = params$par[2])))
    } else if (distribucion == "lognormal") {
      return((plnorm(log(limites[[i]][2]), params$par[1], params$par[2]) - 
              plnorm(log(limites[[i]][1]), params$par[1], params$par[2])) / 
              (1 - plnorm(log(deducible), params$par[1], params$par[2])))
    } else if (distribucion == "pareto") {
      return((ppareto(limites[[i]][2], params$par[1], params$par[2]) - 
              ppareto(limites[[i]][1], params$par[1], params$par[2])) / 
              (1 - ppareto(deducible, params$par[1], params$par[2])))
    } else {
      stop("Distribución no soportada")
    }
  })
  return(p * total_observaciones)
}


total_observaciones <- sum(sevDatos$Frecuencia)

expected_weibull <- calcular_frecuencias_esperadas("weibull", params_weibull, limites, total_observaciones)
expected_gamma <- calcular_frecuencias_esperadas("gamma", params_gamma, limites, total_observaciones)
expected_lognormal <- calcular_frecuencias_esperadas("lognormal", params_lognormal, limites, total_observaciones)
expected_pareto <- calcular_frecuencias_esperadas("pareto", params_pareto, limites, total_observaciones)

# Realizar la prueba chi cuadrado para cada distribución
observed <- sevDatos$Frecuencia

chisq_test_weibull <- chisq.test(x = observed, p = expected_weibull / sum(expected_weibull), rescale.p = TRUE)
chisq_test_gamma <- chisq.test(x = observed, p = expected_gamma / sum(expected_gamma), rescale.p = TRUE)
chisq_test_lognormal <- chisq.test(x = observed, p = expected_lognormal / sum(expected_lognormal), rescale.p = TRUE)
chisq_test_pareto <- chisq.test(x = observed, p = expected_pareto / sum(expected_pareto), rescale.p = TRUE)

# Mostrar los resultados de las pruebas chi cuadrado
print(chisq_test_weibull)
print(chisq_test_gamma)
print(chisq_test_lognormal)
print(chisq_test_pareto)


# Crear un data frame con las frecuencias observadas y esperadas
comparacion <- data.frame(
  Rango = rep(sevDatos$Rango, 5),
  Frecuencia = c(sevDatos$Frecuencia, expected_weibull, expected_gamma, expected_lognormal, expected_pareto),
  Tipo = rep(c("Observada", "Weibull", "Gamma", "Lognormal", "Pareto"), each = nrow(sevDatos))
)

# Graficar las frecuencias observadas y esperadas
ggplot(comparacion, aes(x = Rango, y = Frecuencia, fill = Tipo)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparación de Frecuencias Observadas y Esperadas",
       x = "Rango",
       y = "Frecuencia",
       fill = "Distribución") +
  theme_minimal() +
  scale_fill_manual(values = c("Observada" = "black", "Weibull" = "blue", "Gamma" = "red", "Lognormal" = "green", "Pareto" = "purple"))

x_values <- seq(0, 6000, length.out = 1000)
# Calcular las densidades para las distribuciones Pareto y Lognormal
density_pareto <- dpareto(x_values, params_pareto$par[1], params_pareto$par[2])
density_lognormal <- dlnorm(x_values, params_lognormal$par[1], params_lognormal$par[2])

# Crear un data frame para las densidades
density_data <- data.frame(
  x_values = rep(x_values, 2),
  Density = c(density_pareto, density_lognormal),
  Distribucion = rep(c("Pareto", "Lognormal"), each = length(x_values))
)

# Graficar las densidades de Pareto y Lognormal
ggplot(density_data, aes(x = x_values, y = Density, color = Distribucion)) +
  geom_line(size = 1) +
  labs(title = "Densidades Ajustadas de las Distribuciones Pareto y Lognormal",
       x = "Valores",
       y = "Densidad",
       color = "Distribución") +
  theme_minimal() +
  scale_color_manual(values = c("Pareto" = "purple", "Lognormal" = "green"))


```



## Pareto

```{r}
esperanza_pareto <-  params_pareto$par[2] / (params_pareto$par[1] - 1)

severidad <- esperanza_pareto/mean(unique(mydata$Intermedio))


```

## Lognormal

```{r}
esperanza_lognormal <- exp(params_lognormal$par[1] + params_lognormal$par[2]^2/2)

severidad <- exp(esperanza_lognormal)/mean(unique(mydata$Intermedio))

```

## Weibull

```{r}

esperanza_weibull <- params_weibull$par[2] * gamma(1 + 1/params_weibull$par[1])

severidad <- esperanza_weibull/mean(unique(mydata$Intermedio))

```







